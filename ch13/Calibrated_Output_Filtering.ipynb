{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Post-hoc: Calibrated Output Filtering for Safer LLM Responses\n",
        "In this notebook, we implement a post-hoc mitigation strategy to filter or rephrase potentially toxic outputs generated by large language models (LLMs). This is an essential safety layer in responsible AI deployment."
      ],
      "metadata": {
        "id": "ZO7CJ6tvt_1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Install Required Packages\n",
        "Make sure you have the required libraries installed."
      ],
      "metadata": {
        "id": "-zn4povauDxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers detoxify torch"
      ],
      "metadata": {
        "id": "d0CAeYW9uE4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Import Libraries\n",
        "We use Hugging Face's transformers for text generation and Detoxify for toxicity detection."
      ],
      "metadata": {
        "id": "h8b8Zwp0uKLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from detoxify import Detoxify"
      ],
      "metadata": {
        "id": "x3yW1pizuJ35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Initialize Models\n",
        "We load a large language model (gpt2-xl) and the Detoxify model for toxicity scoring."
      ],
      "metadata": {
        "id": "x8tq2XvfuP5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GPT-2 XL for text generation\n",
        "generator = pipeline('text-generation', model='gpt2-xl')\n",
        "\n",
        "# Load Detoxify toxicity detection model\n",
        "detox = Detoxify('original')"
      ],
      "metadata": {
        "id": "VeRONis8uPg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Define Bias Correction Logic (Placeholder)\n",
        "You can define any logic here to modify biased or inappropriate output, such as rephrasing or tone adjustment."
      ],
      "metadata": {
        "id": "Nw0tncK_ub0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder for any custom bias correction (e.g., rephrasing toxic text)\n",
        "def apply_bias_correction(text):\n",
        "    return text  # Return original for now\n"
      ],
      "metadata": {
        "id": "82M_M6zuublu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Define the Safe Text Generation Function\n",
        "This function wraps generation with toxicity detection and conditional filtering."
      ],
      "metadata": {
        "id": "nxBDHpPduhEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_generate(prompt):\n",
        "    # Generate text from the LLM\n",
        "    raw_output = generator(prompt, max_length=100)[0]['generated_text']\n",
        "\n",
        "    # Check toxicity using Detoxify\n",
        "    toxicity_score = detox.predict(raw_output)['toxicity']\n",
        "\n",
        "    # If toxicity is above threshold, block response\n",
        "    if toxicity_score > 0.7:\n",
        "        return \"I cannot generate this content responsibly.\"\n",
        "\n",
        "    # Optionally apply bias correction\n",
        "    return apply_bias_correction(raw_output)"
      ],
      "metadata": {
        "id": "sS3d0jkNuhek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Step 6: Test the Safe Generation\n",
        "Letâ€™s try with both safe and unsafe prompts."
      ],
      "metadata": {
        "id": "wUz67zhbunI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Safe prompt\n",
        "print(safe_generate(\"Once upon a time in a peaceful village\"))"
      ],
      "metadata": {
        "id": "UjQGX4U-ubjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(safe_generate(\"Write a hateful speech\"))"
      ],
      "metadata": {
        "id": "U2nfdsPCubgx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}