{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Interpret BERT with LayerIntegratedGradients (Captum)\n",
        "This notebook demonstrates how to use Captum's LayerIntegratedGradients to compute input feature attributions for a BERT model performing sentiment classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "nlw40wi9mA0c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcmC94-olvfc"
      },
      "outputs": [],
      "source": [
        "! pip install transformers captum torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Required Libraries"
      ],
      "metadata": {
        "id": "seAAZHGSmFTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from captum.attr import LayerIntegratedGradients\n",
        "import torch"
      ],
      "metadata": {
        "id": "8-qeoiwAmJPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Load Pretrained BERT Model and Tokenizer\n",
        "We'll use the bert-base-uncased model and tokenizer from Hugging Face Transformers."
      ],
      "metadata": {
        "id": "H7JxU2YBmNOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text for sentiment analysis\n",
        "text = \"This is a great movie!\"\n",
        "\n",
        "# Tokenize the text into input IDs and attention masks\n",
        "inputs = tokenizer(text, return_tensors='pt')\n",
        "input_ids = inputs['input_ids']\n",
        "attention_mask = inputs['attention_mask']\n"
      ],
      "metadata": {
        "id": "b1Nm8leumQAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Extract Input Embeddings\n",
        "We get the embeddings from the model's embedding layer and enable gradient computation."
      ],
      "metadata": {
        "id": "zn1M5Dw1mSuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get input embeddings from the BERT embedding layer\n",
        "embedding_layer = model.bert.embeddings\n",
        "input_embeddings = embedding_layer(input_ids)\n",
        "\n",
        "# Enable gradients for input embeddings\n",
        "input_embeddings.requires_grad_()\n"
      ],
      "metadata": {
        "id": "fJqB-GJvmYCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Define a Custom Forward Function\n",
        "Captum needs a function that maps embeddings to outputs. We'll define that here."
      ],
      "metadata": {
        "id": "cx61eVezmYvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom forward function that accepts input embeddings\n",
        "def custom_forward(embeds):\n",
        "    outputs = model(inputs_embeds=embeds, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "jzbjSegYmZR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Select Target Prediction Class\n",
        "We choose the class index for which we want to compute attributions.\n",
        "For binary classification, 1 might represent positive sentiment."
      ],
      "metadata": {
        "id": "42xMw57rme97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the target class (e.g., 1 for positive sentiment)\n",
        "target_prediction = 1"
      ],
      "metadata": {
        "id": "aeBxh1SdmfR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Compute Attributions with LayerIntegratedGradients\n",
        "We now use Captum to compute feature attributions for the input embeddings."
      ],
      "metadata": {
        "id": "3RGMaWFrmmla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LayerIntegratedGradients with the embedding layer\n",
        "lig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\n",
        "\n",
        "# Compute attributions for the target prediction\n",
        "attributions = lig.attribute(inputs=input_embeddings, target=target_prediction)\n"
      ],
      "metadata": {
        "id": "b34bHwKSmoOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "model.eval()\n",
        "\n",
        "# Tokenize input\n",
        "text = \"This is a great movie!\"\n",
        "inputs = tokenizer(text, return_tensors='pt')\n",
        "input_ids = inputs['input_ids']\n",
        "attention_mask = inputs['attention_mask']\n",
        "\n",
        "# Get embeddings from model\n",
        "embedding_layer = model.bert.embeddings\n",
        "input_embeddings = embedding_layer(input_ids)\n",
        "input_embeddings.requires_grad_()\n",
        "\n",
        "# Define a custom forward function to pass embeddings and get prediction\n",
        "def custom_forward(embeds):\n",
        "    outputs = model(inputs_embeds=embeds, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    return logits\n",
        "\n",
        "# Target index (e.g., class index 1 for positive sentiment)\n",
        "target_prediction = 1\n",
        "\n",
        "# Initialize LayerIntegratedGradients\n",
        "lig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)\n",
        "\n",
        "# Compute attributions\n",
        "attributions = lig.attribute(inputs=input_embeddings, target=target_prediction)\n"
      ],
      "metadata": {
        "id": "8qxx11tolzkw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}