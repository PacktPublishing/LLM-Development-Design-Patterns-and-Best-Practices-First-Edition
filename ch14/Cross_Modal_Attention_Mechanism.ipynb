{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Modal Attention Mechanism\n",
        "\n",
        "In this notebook, we implement a Cross-Modal Attention mechanism where text features attend to image features, inspired by architectures like Flamingo.\n",
        "\n",
        "This type of attention allows the model to dynamically route relevant visual information based on textual context â€” similar to how humans align specific words in a question to parts of an image.\n"
      ],
      "metadata": {
        "id": "5HFG0YsfphoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n",
        "We import PyTorch libraries for building the model and numpy for numerical operations.\n"
      ],
      "metadata": {
        "id": "J4RliV5QpkCX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4TRxl_npg-q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CrossModalAttention Class\n",
        "\n",
        "This class implements the cross-modal attention mechanism.\n",
        "\n",
        "- It uses multi-head attention to let text features (queries) attend to image features (keys and values).\n",
        "- The linear layers project inputs into query, key, and value spaces.\n",
        "- Scaled dot-product attention computes similarity scores.\n",
        "- The output is a fused representation combining text and image information.\n"
      ],
      "metadata": {
        "id": "jmB4gwUXpmf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, embed_dim=256, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Linear layers to project text features to queries,\n",
        "        # and image features to keys and values\n",
        "        self.text_query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.image_key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.image_value = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self.output_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n"
      ],
      "metadata": {
        "id": "egxiHRk_pqKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Pass\n",
        "\n",
        "The `forward` method computes the attention output:\n",
        "\n",
        "- Input shapes:\n",
        "  - `text_features`: (batch_size, text_len, embed_dim)\n",
        "  - `image_features`: (batch_size, image_len, embed_dim)\n",
        "\n",
        "- Steps:\n",
        "  1. Project inputs into query (Q), key (K), and value (V) vectors.\n",
        "  2. Reshape and transpose for multi-head attention.\n",
        "  3. Calculate scaled dot-product attention scores.\n",
        "  4. Apply an optional attention mask.\n",
        "  5. Use softmax to get attention weights and apply dropout.\n",
        "  6. Compute weighted sum of the values.\n",
        "  7. Project back to output dimension.\n"
      ],
      "metadata": {
        "id": "Vl8OXKxwpoYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def forward(self, text_features, image_features, attention_mask=None):\n",
        "        batch_size, text_len, _ = text_features.shape\n",
        "        _, image_len, _ = image_features.shape\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.text_query(text_features)      # Queries from text\n",
        "        K = self.image_key(image_features)      # Keys from image\n",
        "        V = self.image_value(image_features)    # Values from image\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(batch_size, text_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, image_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, image_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
        "        if attention_mask is not None:\n",
        "            scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        context = torch.matmul(attention_weights, V)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, text_len, self.embed_dim)\n",
        "\n",
        "        output = self.output_proj(context)\n",
        "        return output, attention_weights\n"
      ],
      "metadata": {
        "id": "wb1p8A8pptZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Usage\n",
        "\n",
        "We create an instance of the model and pass simulated text and image features.\n",
        "\n",
        "- Text features: batch of 4 samples, 20 tokens each, embedding size 256.\n",
        "- Image features: batch of 4 samples, 196 image patches (14x14 grid), embedding size 256.\n",
        "\n",
        "The output includes attended features and attention weights.\n"
      ],
      "metadata": {
        "id": "mvKJ5dorpwGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "cross_attention = CrossModalAttention(embed_dim=256, num_heads=8)\n",
        "\n",
        "# Simulated input data\n",
        "text_features = torch.randn(4, 20, 256)    # 4 samples, 20 text tokens\n",
        "image_features = torch.randn(4, 196, 256)  # 4 samples, 196 image patches (14x14)\n",
        "\n",
        "# Forward pass\n",
        "attended_features, attention_weights = cross_attention(text_features, image_features)\n",
        "\n",
        "# Inspect outputs\n",
        "print(f\"Cross-modal attention output shape: {attended_features.shape}\")\n",
        "print(f\"Attention weights shape: {attention_weights.shape}\")\n"
      ],
      "metadata": {
        "id": "rUp_QmkVpyUI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}