{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization in Transformer Models: FP16 Example"
      ],
      "metadata": {
        "id": "qVQwoyWqvjcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Quantization?\n",
        "\n",
        "Quantization is a model compression technique that reduces the numerical precision of neural network weights and activations. Rather than using the default 32-bit floating point (FP32), we can use formats like:\n",
        "\n",
        "- FP16 (16-bit floating point)\n",
        "- INT8 (8-bit integer)\n",
        "- 4-bit formats\n",
        "\n",
        "### Why Use Quantization?\n",
        "\n",
        "- **Faster inference**: Lower-precision math is faster on modern hardware.\n",
        "- **Less memory usage**: Models take up less space.\n",
        "- **Lower power consumption**: Efficient for edge and mobile devices.\n"
      ],
      "metadata": {
        "id": "ii7PHT3mUpX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FP16 Quantization\n",
        "\n",
        "**Half-Precision Floating Point (FP16)** is a commonly used quantization format. It represents floating-point numbers using only 16 bits.\n",
        "\n",
        "### Benefits of FP16:\n",
        "-  Keeps a wide dynamic range of values.\n",
        "-  Accelerates matrix operations on GPUs (especially NVIDIA Tensor Cores).\n",
        "-  Minimal to no accuracy degradation compared to FP32.\n",
        "-  Easily supported in libraries like Hugging Face Transformers and PyTorch.\n"
      ],
      "metadata": {
        "id": "B-p65hXlUymM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up the Environment\n",
        "\n",
        "Before running the code:\n",
        "- Make sure `transformers` and `torch` are installed.\n",
        "- A GPU with FP16 support (e.g., NVIDIA Turing or Ampere GPUs) is recommended.\n"
      ],
      "metadata": {
        "id": "UA__jQr-U2uS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IJuPFhgiZx-X"
      },
      "outputs": [],
      "source": [
        "# Install Hugging Face Transformers and PyTorch if not already installed\n",
        "# Uncomment the next line if needed\n",
        "# !pip install transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Model in FP16"
      ],
      "metadata": {
        "id": "GQ_LEZeRU5pS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Configuration\n",
        "model_name = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
        "token = \"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\"  # WARNING: Use environment variables in production\n",
        "\n",
        "# Load the model with FP16 precision and automatic GPU/CPU allocation\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,  # Use FP16 quantization\n",
        "    device_map=\"auto\",          # Automatically use GPU if available\n",
        "    token=token                 # Hugging Face Hub access token\n",
        ")\n",
        "\n",
        "# Load the tokenizer (converts text to tokens and back)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    token=token\n",
        ")\n"
      ],
      "metadata": {
        "id": "YjHwAKhJU5K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inference with FP16 Model"
      ],
      "metadata": {
        "id": "ep0q67niVmj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the input prompt\n",
        "input_text = \"Explain the transformer architecture\"\n",
        "\n",
        "# Tokenize and move tensors to GPU\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate model response (text generation)\n",
        "outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "\n",
        "# Decode token IDs into readable text\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(response)"
      ],
      "metadata": {
        "id": "qznWF5tgVm7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "Using FP16 precision provides a solid balance between performance and accuracy:\n",
        "\n",
        "- Reduces GPU memory usage, allowing for larger models or batch sizes.\n",
        "- Inference is faster thanks to optimized GPU operations.\n",
        "- Accuracy remains very close to that of FP32 models.\n",
        "- Integration is simple using the Hugging Face Transformers library with `torch_dtype=torch.float16`.\n",
        "\n",
        "This makes FP16 a go-to solution for production inference on powerful GPUs.\n",
        "\n",
        "In the next section, we'll explore more aggressive quantization using **INT8** for deployments on mobile or edge devices.\n"
      ],
      "metadata": {
        "id": "5FEuc7FQWGe1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bWL03yUXWKks"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}