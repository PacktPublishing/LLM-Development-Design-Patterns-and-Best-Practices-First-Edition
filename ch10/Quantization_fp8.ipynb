{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOvRu-rmWb3Q"
      },
      "source": [
        "# 8-bit Integer Quantization (INT8) for Transformer Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRuuNIWkWkpt"
      },
      "source": [
        "## What is 8-bit (INT8) Quantization?\n",
        "\n",
        "INT8 quantization compresses both model **weights** and **activations** to 8-bit integers.\n",
        "\n",
        "This results in:\n",
        "-  Faster inference speeds (especially on CPUs and low-power GPUs)\n",
        "-  Smaller memory footprint (4x smaller than FP32)\n",
        "-  Better energy efficiency for edge or mobile devices\n",
        "\n",
        "###  Trade-off\n",
        "Slight accuracy degradation may occur, particularly in sensitive layers. Techniques like **outlier thresholding** help reduce that.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZBtTGMlWpTZ"
      },
      "source": [
        "## Why Use INT8 Quantization?\n",
        "\n",
        "- **Edge/Mobile Optimization**: Perfect for devices with limited RAM/compute.\n",
        "- **Compression**: Reduces model size dramatically â€” ~4x smaller than FP32.\n",
        "- **Latency**: Speeds up inference, especially on INT8-supported hardware.\n",
        "- **Simple Integration**: Easily enabled via Hugging Face + bitsandbytes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M99-y_qWuKH"
      },
      "source": [
        "### Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmI9OnndWa4Y"
      },
      "outputs": [],
      "source": [
        "!pip install transformers bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02gtS88gXA2r"
      },
      "source": [
        "### Configure INT8 Quantization Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHo0nRMPXB4_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Set 8-bit quantization parameters\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,             # Enables 8-bit quantization\n",
        "    llm_int8_threshold=6.0         # Handles outliers in sensitive layers (higher = more aggressive quantization)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHnxgdJTXJ4l"
      },
      "source": [
        "### Load Quantized Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxyBTKk8XOla"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"deepseek-ai/deepseek-coder-6.7b-instruct\",\n",
        "    quantization_config=bnb_config,  # Apply 8-bit config\n",
        "    device_map=\"auto\"                # Automatically allocate across available GPUs/CPUs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eutaFqZ9XQzy"
      },
      "source": [
        "### Memory Footprint Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCtypTttXToz"
      },
      "outputs": [],
      "source": [
        "# Check memory usage of the loaded model (in GB)\n",
        "model_size_gb = model.get_memory_footprint() / 1e9\n",
        "print(f\"Estimated 8-bit model size: {model_size_gb:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzhjP10sXWMS"
      },
      "source": [
        "##  Summary: INT8 Quantization Results\n",
        "\n",
        "-  **4x Smaller**: Dramatically reduces memory usage vs FP32\n",
        "-  **Fast Inference**: Especially on modern CPUs and Tensor Cores\n",
        "-  **Outlier Robust**: `llm_int8_threshold` maintains stability\n",
        "-  **Simple to Enable**: Just use `BitsAndBytesConfig` with Hugging Face\n",
        "\n",
        "###  Recommended Use Cases:\n",
        "- Deployment on edge/mobile devices\n",
        "- Latency-critical inference services\n",
        "- GPU-constrained production environments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gipg0GZWXjF6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
