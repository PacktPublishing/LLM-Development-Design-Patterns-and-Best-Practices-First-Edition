{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Audits\n",
        "\n",
        "In deploying large language models (LLMs) that utilize Retrieval-Augmented Generation (RAG) techniques, it’s essential to establish auditing mechanisms to ensure the consistency and accuracy of responses. The following function, audit_llm_response, serves as a basic auditing tool that compares the LLM’s output with trusted source data. By logging the success or failure of each audit check with a timestamp, this function provides a simple yet effective method to monitor data consistency, helping to detect discrepancies that could impact decision-making and model reliability."
      ],
      "metadata": {
        "id": "0Dv7IDbWaAu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Define the audit_llm_response Function"
      ],
      "metadata": {
        "id": "x--Z8bruaI72"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "seB579xoZ9OC"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "\n",
        "def audit_llm_response(response, source_data):\n",
        "   \"\"\"\n",
        "   Compares LLM response with source data for audit purposes.\n",
        "   \"\"\"\n",
        "   current_time = datetime.datetime.now()\n",
        "   # Audit check: Is the response data consistent with source data?\n",
        "   if response == source_data:\n",
        "       print(f\"Audit successful at {current_time}: Data is up-to-date and accurate.\")\n",
        "   else:\n",
        "       print(f\"Audit failed at {current_time}: Mismatch found in response data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So let’s try the audit_llm_response  function, so let’s Imagine we have an LLM that retrieves the latest stock prices using a RAG database, and we want to verify that the output matches the expected data from a trusted API or data source. Here’s how this function could be used in such a context:"
      ],
      "metadata": {
        "id": "Xnotwf-HaM2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample source data from trusted RAG source\n",
        "source_data = \"Stock price of XYZ is $200\"\n",
        "\n",
        "\n",
        "# Sample LLM response\n",
        "response = \"Stock price of XYZ is $200\"\n",
        "\n",
        "\n",
        "wrong_response = \"Stock price of XYZ is $198\"\n",
        "\n",
        "\n",
        "# Run the audit function to verify consistency\n",
        "audit_llm_response(response, source_data)\n",
        "\n",
        "\n",
        "# Run the audit function to verify consistency\n",
        "audit_llm_response(response, wrong_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW0foUreaO5p",
        "outputId": "f8c99989-73fc-40d1-d710-a761bb3e1d77"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audit successful at 2024-11-18 13:14:49.291193: Data is up-to-date and accurate.\n",
            "Audit failed at 2024-11-18 13:14:49.291835: Mismatch found in response data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compliance Auditing\n",
        "\n",
        "compliance with regulatory standards is crucial. The compliance_audit function logs each interaction between the model and its RAG data sources, creating a traceable record of source and response pairs. By logging these interactions in a dedicated compliance file, organizations can monitor data usage and verify adherence to legal and regulatory requirements, thus maintaining transparency and accountability in LLM deployments."
      ],
      "metadata": {
        "id": "hPTHpvzAaYi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "def compliance_audit(source, response):\n",
        "   \"\"\"\n",
        "   Logs each interaction with the RAG source for compliance tracking.\n",
        "   \"\"\"\n",
        "   print(f\"Source: {source}, Response: {response}, Compliance Check: Passed\")"
      ],
      "metadata": {
        "id": "Ci5R7aD0aRH1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's try this the complaince_audit function"
      ],
      "metadata": {
        "id": "bNASV005asIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Log interaction with a knowledge database\n",
        "source_data_1 = \"https://knowledgebase.example.com/article/123\"\n",
        "response_1 = \"According to the knowledge database, AI techniques are advancing rapidly.\"\n",
        "compliance_audit(source_data_1, response_1)\n",
        "\n",
        "\n",
        "# Example 2: Log interaction with a financial report API\n",
        "source_data_2 = \"https://financialdata.example.com/reports/Q3_2024\"\n",
        "response_2 = \"The Q3 financial report shows a 12% increase in revenue.\"\n",
        "compliance_audit(source_data_2, response_2)\n",
        "\n",
        "\n",
        "# Example 3: Log interaction with a healthcare dataset\n",
        "source_data_3 = \"https://healthdata.example.com/patient/456\"\n",
        "response_3 = \"Patient 456 has a recorded history of hypertension.\"\n",
        "compliance_audit(source_data_3, response_3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF9HtOw0ao8F",
        "outputId": "d52f825b-1389-4a4d-eef3-c59d93c2e3a2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source: https://knowledgebase.example.com/article/123, Response: According to the knowledge database, AI techniques are advancing rapidly., Compliance Check: Passed\n",
            "Source: https://financialdata.example.com/reports/Q3_2024, Response: The Q3 financial report shows a 12% increase in revenue., Compliance Check: Passed\n",
            "Source: https://healthdata.example.com/patient/456, Response: Patient 456 has a recorded history of hypertension., Compliance Check: Passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feedback Mechanisms\n",
        "\n",
        "gathering user feedback on model responses is essential for continuous improvement. Feedback mechanisms enable teams to refine model responses, enhance relevance, and reduce any misunderstandings that might arise in sensitive contexts. The following code snippet defines a simple feedback logging system, which stores feedback entries in a list for review and analysis. This structure allows developers to track user responses and identify patterns in the feedback, guiding targeted adjustments to the LLM and its response-generation strategies.\n"
      ],
      "metadata": {
        "id": "yC-eqjHSa1g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedback_log = []\n",
        "\n",
        "\n",
        "def gather_feedback(response, feedback):\n",
        "   \"\"\"\n",
        "   Collects user feedback on LLM responses for continuous improvement.\n",
        "   \"\"\"\n",
        "   feedback_log.append({\"response\": response, \"feedback\": feedback})\n",
        "   print(\"Feedback collected and stored.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "AqIc-M_Tax_C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each example call to gather_feedback adds an entry to the feedback_log list, which stores feedback on the given response for later analysis. This collected feedback provides insights for model refinement and enables the improvement of response quality based on user input. The feedback_log helps maintain a feedback loop, essential in systems using LLMs with RAG by identifying areas where responses need clarity, specificity, or additional information.\n"
      ],
      "metadata": {
        "id": "dwtDWFbQa-rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Collect feedback on a response related to general AI information\n",
        "response_1 = \"AI technologies are evolving and improving across industries.\"\n",
        "feedback_1 = \"This response is accurate but could include examples of specific industries.\"\n",
        "gather_feedback(response_1, feedback_1)\n",
        "\n",
        "\n",
        "# Example 2: Collect feedback on a response from a medical knowledge base\n",
        "response_2 = \"Patient treatment protocols are standardized to improve outcomes.\"\n",
        "feedback_2 = \"Please clarify which protocols are being referenced.\"\n",
        "gather_feedback(response_2, feedback_2)\n",
        "\n",
        "\n",
        "# Example 3: Collect feedback on a response regarding financial trends\n",
        "response_3 = \"The stock market saw significant growth in Q3.\"\n",
        "feedback_3 = \"Response is too general. Needs more specific data points.\"\n",
        "gather_feedback(response_3, feedback_3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncGuQAq0a-T0",
        "outputId": "816fca30-4c3d-4853-e5f9-c0c08521a826"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feedback collected and stored.\n",
            "Feedback collected and stored.\n",
            "Feedback collected and stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c6NAa5XSbBdC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}