{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain & LlamaIndex for Multi-LLM Coordination\n",
        "\n",
        "**LangChain** and **LlamaIndex** are powerful frameworks enabling sophisticated orchestration and multi-LLM pipelines, especially for knowledge-intensive domains.\n",
        "\n",
        "---\n",
        "\n",
        "## LangChain: Agentic Workflow for Model Specialization\n",
        "\n",
        "LangChain enables **agentic workflows** where different LLMs specialize in parts of a task.  \n",
        "For example, GPT-4 might do reasoning, while Claude or LLaMA handles summarization.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Agent System**: Routes tasks to specialized models (e.g., summarizer, analyzer).\n",
        "- **Memory Management**: Maintains state across LLM calls.\n",
        "- **Tool Use**: Wrap models as `Tool` with function and description.\n",
        "\n",
        "---\n",
        "\n",
        "### Installation"
      ],
      "metadata": {
        "id": "Gjtn-G3Bd9f2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBg9zGvSd9Da"
      },
      "outputs": [],
      "source": [
        "! pip install langchain langchain-community llama-index\n",
        "! pip install ollama\n",
        "! ollama pull deepseek-llm\n",
        "! ollama pull llama3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install huggingface_hub\n",
        "! huggingface-cli login\n"
      ],
      "metadata": {
        "id": "K6HTtJ1EeRZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-LLM Workflow with LangChain"
      ],
      "metadata": {
        "id": "sA7y4gxreLdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "\n",
        "# Initialize LLMs\n",
        "deepseek = Ollama(model=\"deepseek-llm\")  # Technical reasoning\n",
        "llama3 = Ollama(model=\"llama3\")          # Concise summarization\n",
        "\n",
        "# Define tools\n",
        "tools = [\n",
        "   Tool(name=\"DeepSeek_Analysis\", func=deepseek, description=\"Technical analysis\"),\n",
        "   Tool(name=\"Llama3_Summarization\", func=llama3, description=\"Summarization\")\n",
        "]\n",
        "\n",
        "# Coordinate using an agent\n",
        "agent = initialize_agent(tools, llama3, agent=\"conversational-react-description\")\n",
        "result = agent.run(\"Analyze the impact of rising interest rates on tech stocks, then summarize.\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "VD7-Ysf5eTie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LlamaIndex: Hierarchical Multi-LLM Synthesis\n",
        "LlamaIndex (formerly GPT Index) complements LangChain by enabling multi-layered pipelines, where different LLMs operate at different abstraction levels.\n",
        "\n",
        "#### Architecture:\n",
        "Base Layer: Processes raw chunks (e.g., extract metrics)\n",
        "\n",
        "Synthesis Layer: Aggregates results into higher-level insights\n",
        "\n"
      ],
      "metadata": {
        "id": "fjAjo4EnebKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.llms import LangChainLLM\n",
        "\n",
        "# Load your documents\n",
        "documents = SimpleDirectoryReader(\"financial_reports\").load_data()\n",
        "\n",
        "# Wrap LLMs\n",
        "base_llm = LangChainLLM(llm=deepseek)\n",
        "synthesis_llm = LangChainLLM(llm=llama3)\n",
        "\n",
        "# Create context and index\n",
        "service_context = ServiceContext.from_defaults(llm=base_llm)\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "# Run query through layered pipeline\n",
        "response = index.as_query_engine(llm=synthesis_llm).query(\"Compare Q3 performance across companies.\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "2negbxWrebwC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}